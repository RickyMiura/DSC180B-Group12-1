{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b70891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "from torch_geometric.data import InMemoryDataset, Data\n",
    "from shutil import copyfile\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287b834",
   "metadata": {},
   "outputs": [],
   "source": [
    "i8_vds_list = ['1111514 E',\n",
    "'1108728 E',\n",
    "'1114540 E',\n",
    "'1114546 W',\n",
    "'1113486 E',\n",
    "'1116574 W',\n",
    "'1115356 E',\n",
    "'1115528 W',\n",
    "'1115413 E',\n",
    "'1115517 W',\n",
    "'1115432 E',\n",
    "'1115555 W',\n",
    "'1115438 E',\n",
    "'1115565 W',\n",
    "'1111551 E',\n",
    "'1111552 W',\n",
    "'1115420 E',\n",
    "'1116165 W',\n",
    "'1115426 E',\n",
    "'1115548 W',\n",
    "'1111566 E',\n",
    "'1111540 W',\n",
    "'1115444 E',\n",
    "'1115572 W',\n",
    "'1116783 E',\n",
    "'1111541 W',\n",
    "'1111565 E',\n",
    "'1115578 W',\n",
    "'1111549 E',\n",
    "'1111550 W',\n",
    "'1122623 W',\n",
    "'1115450 E',\n",
    "'1115584 W',\n",
    "'1108366 E',\n",
    "'1115592 W',\n",
    "'1127052 W',\n",
    "'1118752 E',\n",
    "'1118760 W',\n",
    "'1108333 W',\n",
    "'1111564 E',\n",
    "'1114573 E',\n",
    "'1127026 W',\n",
    "'1111535 E',\n",
    "'1108341 W',\n",
    "'1108343 W',\n",
    "'1111534 E',\n",
    "'1108345 W',\n",
    "'1108423 W',\n",
    "'1108347 W',\n",
    "'1115463 E',\n",
    "'1115608 W',\n",
    "'1111530 E',\n",
    "'1108385 W',\n",
    "'1111532 E',\n",
    "'1108351 W',\n",
    "'1111531 E',\n",
    "'1111561 W',\n",
    "'1122637 E',\n",
    "'1122646 E',\n",
    "'1108387 W',\n",
    "'1108353 W',\n",
    "'1111563 W',\n",
    "'1115612 W',\n",
    "'1115616 W',\n",
    "'1111569 W',\n",
    "'1115477 E',\n",
    "'1116593 E',\n",
    "'1111575 W',\n",
    "'1115624 E',\n",
    "'1115628 E',\n",
    "'1113364 W'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26b5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for vds in i8_vds_list:\n",
    "    vds_id = vds.split()[0] # Sensor ID\n",
    "    vds_dir = vds.split()[1] # Sensor Direction\n",
    "    \n",
    "    # Filepath for each week\n",
    "    w1_file = vds_id + '_I8' + vds_dir + '_W1' + '.csv' \n",
    "    w2_file = vds_id + '_I8' + vds_dir + '_W2' + '.csv'\n",
    "    \n",
    "    # Load in dataset for each week\n",
    "    w1_df = pd.read_csv('../datasets/sensor_speeds/PEMS_I8/'+w1_file)\n",
    "    w2_df = pd.read_csv('../datasets/sensor_speeds/PEMS_I8/'+w2_file)\n",
    "    \n",
    "    # Check that both datasets contain 1 weeks worth of 5 min intervals (1 day = 288 intervals * 7 days = 2016)\n",
    "    if (len(w1_df) != 2016) or (len(w2_df) != 2016):\n",
    "        print(vds_id + ' does not contain all times')\n",
    "        continue\n",
    "    \n",
    "    # Create row representing all speeds for one sensor\n",
    "    row = [int(vds_id)] + list(w1_df['Speed (mph)']) + list(w2_df['Speed (mph)'])\n",
    "    \n",
    "    data.append(row)\n",
    "\n",
    "time_ints = list(w1_df['5 Minutes']) + list(w2_df['5 Minutes']) # Get all time intervals to use as columns\n",
    "cols = ['vds_id'] + time_ints\n",
    "\n",
    "sensor_speed = pd.DataFrame(data, columns=cols).set_index('vds_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150dbcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10b0781",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_pos = pd.read_csv('../datasets/sensor_positions/I8_vds_pos.csv').set_index('vds_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c381cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find distance (in miles) between two coordinates \n",
    "# Source: https://stackoverflow.com/questions/27928/calculate-distance-between-two-latitude-longitude-points-haversine-formula\n",
    "from math import cos, asin, sqrt, pi\n",
    "\n",
    "def distance(lat1, lon1, lat2, lon2):\n",
    "    r = 3956 # miles\n",
    "    p = pi / 180\n",
    "\n",
    "    a = 0.5 - cos((lat2-lat1)*p)/2 + cos(lat1*p) * cos(lat2*p) * (1-cos((lon2-lon1)*p))/2\n",
    "    return 2 * r * asin(sqrt(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e5583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_list = list(sensor_pos.index)\n",
    "sensor_dist = pd.DataFrame(index=sensor_list, columns=sensor_list) # Empty dataframe with sensors as index and columns\n",
    "# Find distances (miles) for all pairs of sensors\n",
    "for sen1 in sensor_list:\n",
    "    for sen2 in sensor_list:\n",
    "        if sen1 == sen2:\n",
    "            sensor_dist.loc[sen1, sen2] = 0.0\n",
    "            continue\n",
    "            \n",
    "        sen1_lat = sensor_pos.loc[sen1, 'Lat']\n",
    "        sen1_lon = sensor_pos.loc[sen1, 'Lng']\n",
    "        sen2_lat = sensor_pos.loc[sen2, 'Lat']\n",
    "        sen2_lon = sensor_pos.loc[sen2, 'Lng']\n",
    "        \n",
    "        sensor_dist.loc[sen1, sen2] = distance(sen1_lat, sen1_lon, sen2_lat, sen2_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d1e287",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028badd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor_conn = pd.DataFrame(index=sensor_list, columns=sensor_list)\n",
    "# Find connectivity between all pairs of sensors\n",
    "for sen1 in i8_vds_list:\n",
    "    for sen2 in i8_vds_list:\n",
    "        sen1_vds = int(sen1.split()[0])\n",
    "        sen1_dir = sen1.split()[1]\n",
    "        sen2_vds = int(sen2.split()[0])\n",
    "        sen2_dir = sen2.split()[1]\n",
    "        \n",
    "        if sen1_dir == sen2_dir:\n",
    "            sensor_conn.loc[sen1_vds, sen2_vds] = 1\n",
    "        \n",
    "        else:\n",
    "            sensor_conn.loc[sen1_vds, sen2_vds] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b2d28c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sensor_conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282bb37e-2784-40b5-89c6-b8007a9f2dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_to_weights(dist_df, conn_df):\n",
    "    # Inverse transform distances (1 / d)\n",
    "    dist_array = dist_df.values\n",
    "    dist_array = np.where(dist_array == 0, np.nan, dist_array)\n",
    "    dist_array_inv = 1 / dist_array\n",
    "    dist_array_inv = pd.DataFrame(dist_array_inv).fillna(0).values\n",
    "    \n",
    "    # Mask with directional connectivity\n",
    "    conn_array = conn_df.values\n",
    "    W = dist_array_inv * conn_array\n",
    "    \n",
    "    # Mask with nearest sensor connectivity\n",
    "    near_sen = np.zeros((W.shape[0], W.shape[0]))\n",
    "    for sen in range(W.shape[0]-1):\n",
    "        no_neigh = False\n",
    "        count = 1\n",
    "        while W[sen][sen+count] == 0:\n",
    "            if count == (W.shape[0]-sen-1):\n",
    "                no_neigh = True\n",
    "                break            \n",
    "            count+=1\n",
    "\n",
    "        if no_neigh:\n",
    "            near_sen[sen][sen+count] = 0\n",
    "\n",
    "        else:\n",
    "            near_sen[sen][sen+count] = 1\n",
    "    \n",
    "    near_sen_sym = np.triu(near_sen) + np.triu(near_sen, 1).T # Make symmetric  \n",
    "    W = W * near_sen_sym\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb2e01-5d00-46a1-8769-f01afe28d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = distance_to_weights(sensor_dist, sensor_conn)\n",
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4476d48e",
   "metadata": {},
   "source": [
    "## Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39230762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(x, mean, std):\n",
    "    \"\"\"\n",
    "    Z-score normalization function: $z = (X - \\mu) / \\sigma $,\n",
    "    where z is the z-score, X is the value of the element,\n",
    "    $\\mu$ is the population mean, and $\\sigma$ is the standard deviation.\n",
    "    :param x: torch array, input array to be normalized.\n",
    "    :param mean: float, the value of mean.\n",
    "    :param std: float, the value of standard deviation.\n",
    "    :return: torch array, z-score normalized array.\n",
    "    \"\"\"\n",
    "    return (x - mean) / std\n",
    "\n",
    "def un_z_score(x_normed, mean, std):\n",
    "    \"\"\"\n",
    "    Undo the Z-score calculation\n",
    "    :param x_normed: torch array, input array to be un-normalized.\n",
    "    :param mean: float, the value of mean.\n",
    "    :param std: float, the value of standard deviation.\n",
    "    \"\"\"\n",
    "    return x_normed * std  + mean\n",
    "\n",
    "\n",
    "def MAPE(v, v_):\n",
    "    \"\"\"\n",
    "    Mean absolute percentage error, given as a % (e.g. 99 -> 99%)\n",
    "    :param v: torch array, ground truth.\n",
    "    :param v_: torch array, prediction.\n",
    "    :return: torch scalar, MAPE averages on all elements of input.\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.abs((v_ - v)) /(v + 1e-15) * 100)\n",
    "\n",
    "\n",
    "def RMSE(v, v_):\n",
    "    \"\"\"\n",
    "    Mean squared error.\n",
    "    :param v: torch array, ground truth.\n",
    "    :param v_: torch array, prediction.\n",
    "    :return: torch scalar, RMSE averages on all elements of input.\n",
    "    \"\"\"\n",
    "    return torch.sqrt(torch.mean((v_ - v) ** 2))\n",
    "\n",
    "\n",
    "def MAE(v, v_):\n",
    "    \"\"\"\n",
    "    Mean absolute error.\n",
    "    :param v: torch array, ground truth.\n",
    "    :param v_: torch array, prediction.\n",
    "    :return: torch scalar, MAE averages on all elements of input.\n",
    "    \"\"\"\n",
    "    return torch.mean(torch.abs(v_ - v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe2f2fd",
   "metadata": {},
   "source": [
    "## Creating the Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a97ac13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant config to use throughout\n",
    "config = {\n",
    "    'BATCH_SIZE': 50,\n",
    "    'EPOCHS': 60,\n",
    "    'WEIGHT_DECAY': 5e-5,\n",
    "    'INITIAL_LR': 3e-4,\n",
    "    'CHECKPOINT_DIR': './runs',\n",
    "    'N_PRED': 2,\n",
    "    'N_HIST': 2,\n",
    "    'DROPOUT': 0.2,\n",
    "    # number of possible 5 minute measurements per day\n",
    "    'N_DAY_SLOT': 288,\n",
    "    # number of days worth of data in the dataset\n",
    "    'N_DAYS': 14,\n",
    "    # If false, use GCN paper weight matrix, if true, use GAT paper weight matrix\n",
    "    'USE_GAT_WEIGHTS': True,\n",
    "    'N_NODE': 228,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08596134",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineDataset(InMemoryDataset):\n",
    "    \"\"\"\n",
    "    Dataset for Graph Neural Networks.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, W, root='', transform=None, pre_transform=None):\n",
    "        self.config = config\n",
    "        self.W = W\n",
    "        super().__init__(root, transform, pre_transform)\n",
    "        self.data = sensor_speed\n",
    "        #self.data, self.slices, self.n_node, self.mean, self.std_dev = torch.load(self.processed_paths[0])\n",
    "\n",
    "    def process(self):\n",
    "        \"\"\"\n",
    "        Process the raw datasets into saved .pt dataset for later use.\n",
    "        Note that any self.fields here wont exist if loading straight from the .pt file\n",
    "        \"\"\"\n",
    "        # Data Preprocessing and loading\n",
    "        data = sensor_speed.values\n",
    "        # Technically using the validation and test datasets here, but it's fine, would normally get the\n",
    "        # mean and std_dev from a large dataset\n",
    "        mean =  np.mean(data)\n",
    "        std_dev = np.std(data)\n",
    "        data = z_score(data, np.mean(data), np.std(data))\n",
    "\n",
    "        _, n_node = data.shape\n",
    "        n_window = self.config['N_PRED'] + self.config['N_HIST']\n",
    "\n",
    "        # manipulate nxn matrix into 2xnum_edges\n",
    "        edge_index = torch.zeros((2, n_node**2), dtype=torch.long)\n",
    "        # create an edge_attr matrix with our weights  (num_edges x 1) --> our edge features are dim 1\n",
    "        edge_attr = torch.zeros((n_node**2, 1))\n",
    "        num_edges = 0\n",
    "        for i in range(n_node):\n",
    "            for j in range(n_node):\n",
    "                if self.W[i, j] != 0.:\n",
    "                    edge_index[0, num_edges] = i\n",
    "                    edge_index[1, num_edges] = j\n",
    "                    edge_attr[num_edges] = self.W[i, j]\n",
    "                    num_edges += 1\n",
    "        # using resize_ to just keep the first num_edges entries\n",
    "        edge_index = edge_index.resize_(2, num_edges)\n",
    "        edge_attr = edge_attr.resize_(num_edges, 1)\n",
    "\n",
    "        sequences = []\n",
    "        # T x F x N\n",
    "        for i in range(self.config['N_DAYS']):\n",
    "            for j in range(self.config['N_SLOT']):\n",
    "                # for each time point construct a different graph with data object\n",
    "                # Docs here: https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Data\n",
    "                g = Data()\n",
    "                g.__num_nodes__ = n_node\n",
    "\n",
    "                g.edge_index = edge_index\n",
    "                g.edge_attr  = edge_attr\n",
    "\n",
    "                # (F,N) switched to (N,F)\n",
    "                sta = i * self.config['N_DAY_SLOT'] + j\n",
    "                end = sta + n_window\n",
    "                # [21, 228]\n",
    "                full_window = np.swapaxes(data[sta:end, :], 0, 1)\n",
    "                g.x = torch.FloatTensor(full_window[:, 0:self.config['N_HIST']])\n",
    "                g.y = torch.FloatTensor(full_window[:, self.config['N_HIST']::])\n",
    "                sequences += [g]\n",
    "\n",
    "        # Make the actual dataset\n",
    "        data, slices = self.collate(sequences)\n",
    "        torch.save((data, slices, n_node, mean, std_dev), self.processed_paths[0])\n",
    "\n",
    "def get_splits(dataset: BaselineDataset, n_slot, splits):\n",
    "    \"\"\"\n",
    "    Given the data, split it into random subsets of train, val, and test as given by splits\n",
    "    :param dataset: TrafficDataset object to split\n",
    "    :param n_slot: Number of possible sliding windows in a day\n",
    "    :param splits: (train, val, test) ratios\n",
    "    \"\"\"\n",
    "    split_train, split_val, _ = splits\n",
    "    i = n_slot*split_train\n",
    "    j = n_slot*split_val\n",
    "    train = dataset[:i]\n",
    "    val = dataset[i:i+j]\n",
    "    test = dataset[i+j:]\n",
    "\n",
    "    return train, val, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e0eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BaselineDataset(config, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c800a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce761ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de14f954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.zeros((2, 2**2), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3484f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr = torch.zeros((2**2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d3430b",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d28db-0210-4f4d-b567-5273951c9e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = sensor_dist.iloc[:3,:3]\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92e4e46-3d08-41b8-b0dc-9464d984cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = W.to_numpy()\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4aab7f6-8cad-4543-b99a-4b02de8148c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_node = 3\n",
    "edge_index = torch.zeros((2, n_node**2), dtype=torch.long)\n",
    "# create an edge_attr matrix with our weights  (num_edges x 1) --> our edge features are dim 1\n",
    "edge_attr = torch.zeros((n_node**2, 1))\n",
    "num_edges = 0\n",
    "for i in range(n_node):\n",
    "    for j in range(n_node):\n",
    "        if W[i, j] != 0.:\n",
    "            edge_index[0, num_edges] = i\n",
    "            edge_index[1, num_edges] = j\n",
    "            edge_attr[num_edges] = W[i, j]\n",
    "            num_edges += 1\n",
    "# using resize_ to just keep the first num_edges entries\n",
    "# edge_index = edge_index.resize_(2, num_edges)\n",
    "edge_attr = edge_attr.resize_(num_edges, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5700568-ac81-48ae-ab6f-b8ee3a3a5603",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0d16ff-38a6-4594-88c5-a2ed0632b810",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d89a287-2a6c-471f-9222-2731a1c25df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d9a3a-df76-4674-8061-ede747c1c96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e13077-3aef-4b26-9b6e-6a8e49099839",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(edge_index.tolist()[0])):\n",
    "    if (edge_index.tolist()[0][i] == 0) and (edge_index.tolist()[1][i] == 0):\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3132fc7f-19fd-4691-9e65-5221d1c53bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x= edge_index.tolist()[0][:6]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea551d02-34ca-4f47-9f5b-86eb2de4d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = edge_index.tolist()[1][:6]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa70bf0-c668-4214-926a-f9b3345cc121",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = torch.IntTensor(x)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8779660-98d6-4e51-894f-5c9d5bd59871",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = torch.IntTensor(y)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b21800-b7bf-4384-b3c9-d663e3da04dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f59c5e-79c6-48a2-bd4b-38dd6163a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = edge_index.tolist()\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a93301-82de-4d36-965d-0b8a0ba0fdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b670412-9902-419c-a11f-ff614b445a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c36c5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e889e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45065592",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
